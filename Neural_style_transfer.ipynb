{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define object : Conserving the content of the original iamge while adopting the style of the reference image. **\n",
    "\n",
    "Below, mathmatically define the Object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Loss(image \\ to \\ transfer) \n",
    "= distance(style(reference_image) - style(generated_image)) + \\\\\n",
    "distance(content(original_image) - content(generated_image))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Distance \n",
    "  -  Norm kind fucntion to minimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Content loss - Consistency between Original and Generated one \n",
    "\n",
    " - would be captured in upper layers \n",
    " - $L2$ norm between the activations of an upper layer in a pretrained convnet, compute over the target image, and the activations of the same layer computedd over the generated image. \n",
    " - Use only a layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Style\n",
    "\n",
    "  - use Multiple layer \n",
    "  - Use $Gram \\ matrix$ of a layer's activation : which is Inner product of the feature maps of a given layer. -> it represents a map of correlations between the layer's feature. These feature corelations capture the statistics of the patterns of a particular spatial scale, which empirically correspond to the appearance of the textures found at this scale. \n",
    "  - aims to preserve similar internal correlations within the activations of different layers, across the style-reference image and the generated image. \n",
    "  - It gurantees that the textures found at different spatial scales look similar across the style-reference image and the generated one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I am doing \n",
    "\n",
    "- Preserve content by maintaing similar high-level layer activations between the target content image and the generated image. The convnet should see both the target iamge, and generated image as containing the same things.\n",
    "\n",
    "- Preserve style by maintaining similar correlations within actvations for both low level layers and high-level layers. \n",
    "> Feature correlations capture textures : generated iamge and the syle reference image should share the same textures at different spatial scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pretrained model - VGG19\n",
    "\n",
    "  1. Setting up a network that computes VGG19 layer activations for the style-feference image, the target image, and the generated image at the same time. \n",
    "\n",
    "  2.  Use the layer activations computed over these three iamges to define the loss function described earlier, which you'll minimize in order to achieve style transfer. \n",
    "  \n",
    "  3. Setting up gradient-descent process to minimize this loss function.\n",
    "  \n",
    "  \n",
    "- We use images which have 400px height fixxed (widely different sizes make style transger more difficult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import vgg19\n",
    "target_image_path = 'target_image.png'\n",
    "style_reference_iamge_path = 'style_reference.png'\n",
    "\n",
    "width, height = load_img(target_image_path).size\n",
    "\n",
    "img_height = 400 \n",
    "img_width = int(width * img_height / height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    # this line to be refactored \n",
    "    img = load_img(image_path, target_size=(img_height, img_width))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img \n",
    "\n",
    "def deprocess_image(x):\n",
    "    \n",
    "    #### where these numbers came from\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    \n",
    "    x = x[:, :, ::-1] # BGR to RGB\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded sucessfully\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "target_image = K.constant(preprocess_image(target_image_path))\n",
    "style_reference_iamge = K.constant(preprocess_image(style_reference_iamge_path))\n",
    "\n",
    "# placeholder that will contain the generated image \n",
    "combination_image = K.placeholder((1, img_height, img_width, 3))\n",
    "\n",
    "# combines the three images in a single batch \n",
    "input_tensor = K.concatenate([target_image, \n",
    "                             style_reference_iamge,\n",
    "                             combination_image], axis=0)\n",
    "\n",
    "\n",
    "model = vgg19.VGG19(input_tensor=input_tensor,\n",
    "                   weights='imagenet', include_top=False)\n",
    "\n",
    "### input_tensor \n",
    "print(\"model loaded sucessfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(base, combination):\n",
    "    return K.sum(K.square(combination - base))\n",
    "\n",
    "def gram_matrix(x):\n",
    "    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = K.dot(features, K.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3 \n",
    "    size = img_height * img_width\n",
    "    \n",
    "    return K.sum(K.square(S - C)) / (4. * (channels**2) * (size**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(x) :\n",
    "    \"\"\"\n",
    "    it operates on the pixels of the generated combination image. \n",
    "    It encourages spatial\n",
    "    \"\"\"\n",
    "    # y축으로의 변화\n",
    "    a = K.square(\n",
    "        x[:, :img_height - 1, :img_width - 1, :] - x[:, 1:, :img_width - 1, :])\n",
    "    \n",
    "    # x축으로의 변화 \n",
    "    b = K.square(\n",
    "        x[:, :img_height - 1, :img_width -1, :] - x[:, :img_height - 1, 1:, :])\n",
    "    \n",
    "    return K.sum(K.pow(a + b, 1.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "\n",
    "# layer for content loss\n",
    "content_layer = 'block5_conv2'\n",
    "\n",
    "# layers for style loss\n",
    "style_layers = ['block1_conv1', \n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "total_variation_weight = 1e-4\n",
    "style_weight = 1.\n",
    "content_weight = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "loss = K.variable(0.) \n",
    "\n",
    "layer_features = outputs_dict[content_layer]\n",
    "target_image_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[2, :, :, :]\n",
    "\n",
    "loss += content_weight * content_loss(target_image_features, combination_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer_name in style_layers:\n",
    "    layer_features = outputs_dict[layer_name]\n",
    "    style_reference_features = layer_features[1, :, :, :]\n",
    "    combination_features = layer_features[2, :, :, :]\n",
    "    s1 = style_loss(style_reference_features, combination_features)\n",
    "    loss += (style_weight / len(style_layers)) * s1\n",
    "    \n",
    "loss += total_variation_weight * total_variation_loss(combination_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = K.gradients(loss, combination_image)[0]\n",
    "\n",
    "fetch_loss_and_grads = K.function([combination_image], [loss, grads])\n",
    "\n",
    "class Evaluator(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "        \n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        x = x.reshape((1, img_height, img_width, 3))\n",
    "        outs = fetch_loss_and_grads([x])\n",
    "        loss_value = outs[0]\n",
    "        grad_values = outs[1].flatten().astype(\"float64\")\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_value = grad_values\n",
    "        return self.loss_value\n",
    "    \n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None \n",
    "        \n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None \n",
    "        return grad_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter : 0\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.misc import imsave \n",
    "import tensorflow as tf\n",
    "import time \n",
    "\n",
    "K.get_session().run(tf.global_variables_initializer())\n",
    "\n",
    "result_prefix = 'result_NST'\n",
    "iterations = 20 \n",
    "\n",
    "x = preprocess_image(target_image_path)\n",
    "\n",
    "x = x.flatten()\n",
    "for i in range(iterations):\n",
    "    print(\"iter : {}\".format(i))\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss,\n",
    "                                    x,\n",
    "                                    fprime=evaluator.grads,\n",
    "                                    maxfun=20)\n",
    "    \n",
    "    print(\"Loss {} : {}\".format(i ,min_val))\n",
    "    img = x.copy().reshape((img_height, img_width, 3))\n",
    "    img = deprocess_image(img)\n",
    "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
    "    \n",
    "    imsave(fname, img)\n",
    "    print('image saved as {}'.format(fname))\n",
    "    end_time = time.time()\n",
    "    print('Iter : {} completed in {}'.format(i, end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
